{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOP4GFAmse3cdB+ugya/Zor"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","import re\n","import json\n","import zipfile\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from typing import Dict, Any, Tuple, List\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_absolute_error, r2_score\n","from sklearn.pipeline import Pipeline\n","\n","import tensorflow as tf\n","from tensorflow.keras import layers, regularizers, optimizers, callbacks, Model"],"metadata":{"id":"2QATsqKV938p","executionInfo":{"status":"ok","timestamp":1757631019748,"user_tz":180,"elapsed":2,"user":{"displayName":"Ignacio Verdugo Montecinos","userId":"08266715628392896686"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"GKtlrZII9k4J","executionInfo":{"status":"ok","timestamp":1757631019764,"user_tz":180,"elapsed":14,"user":{"displayName":"Ignacio Verdugo Montecinos","userId":"08266715628392896686"}}},"outputs":[],"source":["# =========================\n","# 0) CONFIGURACIÓN GENERAL\n","# =========================\n","\n","CSV_PATH = \"./dataset_natalidad.csv\"  # ajusta si es necesario (p.ej. \"/mnt/data/dataset_natalidad.csv\")\n","OUT_DIR = \"./m8_output\"\n","ZIP_PATH = \"./entrega_final_m8.zip\"\n","RNG = 42\n","os.makedirs(OUT_DIR, exist_ok=True)\n","np.random.seed(RNG)\n","tf.random.set_seed(RNG)"]},{"cell_type":"code","source":["# ====================================\n","# 1) CARGA Y EXPLORACIÓN DE LOS DATOS\n","# ====================================\n","\n","def normalize_colnames(cols: List[str]) -> List[str]:\n","    \"\"\"Convierte a snake_case y elimina tildes/espacios para trabajar cómodo.\"\"\"\n","    def _norm(c):\n","        c = c.strip()\n","        # reemplazos básicos\n","        c = c.replace(\"á\",\"a\").replace(\"é\",\"e\").replace(\"í\",\"i\").replace(\"ó\",\"o\").replace(\"ú\",\"u\").replace(\"ñ\",\"n\")\n","        c = re.sub(r\"[^A-Za-z0-9]+\", \"_\", c)\n","        c = re.sub(r\"_+\", \"_\", c).strip(\"_\")\n","        return c.lower()\n","    return [_norm(c) for c in cols]\n","\n","\n","def detect_target(df: pd.DataFrame) -> str:\n","    \"\"\"\n","    Detecta la columna objetivo (tasa de natalidad) de forma robusta.\n","    Busca por patrones comunes: 'tasa_natalidad', 'natalidad', 'birth_rate'.\n","    \"\"\"\n","    lower_map = {c.lower(): c for c in df.columns}\n","    # candidatos explícitos\n","    candidates = [\n","        \"tasa_de_natalidad\", \"tasa_natalidad\", \"natalidad\", \"tasa__natalidad\",\n","        \"birth_rate\", \"births_per_woman\", \"births_per_1000\", \"births_per_1000_people\"\n","    ]\n","    for k in candidates:\n","        if k in lower_map:\n","            return lower_map[k]\n","    # heurística: columna que contenga 'natalid' o 'birth'\n","    for c in df.columns:\n","        cl = c.lower()\n","        if \"natalid\" in cl or \"birth\" in cl:\n","            return c\n","    raise ValueError(\"No se encontró la columna objetivo (tasa de natalidad). Renómbrala o ajusta detect_target().\")\n","\n","\n","def load_dataset(path: str) -> Tuple[pd.DataFrame, str]:\n","    df = pd.read_csv(path)\n","    original_cols = df.columns.tolist()\n","    df.columns = normalize_colnames(df.columns.tolist())\n","    # Mapeo antiguo->nuevo por si quieres rastrear\n","    colmap = dict(zip(original_cols, df.columns))\n","    # detectar objetivo\n","    target_col = detect_target(df)\n","    return df, target_col\n","\n","\n","def basic_eda(df: pd.DataFrame, target: str) -> None:\n","    \"\"\"Distribuciones y matriz de correlaciones (matplotlib puro).\"\"\"\n","    # 1) Histograma de cada columna\n","    fig_dir = OUT_DIR\n","    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n","    # Hist distribuciones\n","    for col in num_cols:\n","        plt.figure()\n","        plt.hist(df[col].dropna().values, bins=20)\n","        plt.title(f\"Distribución de {col}\")\n","        plt.xlabel(col); plt.ylabel(\"Frecuencia\")\n","        plt.tight_layout()\n","        plt.savefig(os.path.join(fig_dir, f\"dist_{col}.png\"), dpi=180)\n","        plt.close()\n","\n","    # 2) Correlaciones\n","    corr = df[num_cols].corr()\n","    plt.figure(figsize=(8, 6))\n","    im = plt.imshow(corr.values, aspect='auto')\n","    plt.colorbar(im, fraction=0.046, pad=0.04)\n","    plt.xticks(range(len(num_cols)), num_cols, rotation=90)\n","    plt.yticks(range(len(num_cols)), num_cols)\n","    plt.title(\"Matriz de correlaciones (Pearson)\")\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(fig_dir, \"correlaciones.png\"), dpi=200)\n","    plt.close()\n","\n","    # Guardar resumen de stats\n","    summary = {\n","        \"shape\": df.shape,\n","        \"columns\": df.columns.tolist(),\n","        \"numeric_columns\": num_cols,\n","        \"describe\": df[num_cols].describe().to_dict()\n","    }\n","    with open(os.path.join(OUT_DIR, \"eda_summary.json\"), \"w\", encoding=\"utf-8\") as f:\n","        json.dump(summary, f, ensure_ascii=False, indent=2)"],"metadata":{"id":"YUXudQq2-BiQ","executionInfo":{"status":"ok","timestamp":1757631019803,"user_tz":180,"elapsed":22,"user":{"displayName":"Ignacio Verdugo Montecinos","userId":"08266715628392896686"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# =============================================\n","# 2) PREPROCESAMIENTO + DISEÑO DEL MODELO (NN)\n","# =============================================\n","\n","def make_preprocessor(feature_cols: List[str]) -> Pipeline:\n","    \"\"\"Pipeline: imputación (mediana) + escalado estándar.\"\"\"\n","    return Pipeline(steps=[\n","        (\"imputer\", SimpleImputer(strategy=\"median\")),\n","        (\"scaler\", StandardScaler())\n","    ])\n","\n","\n","def build_regression_mlp(\n","    n_features: int,\n","    activation: str = \"relu\",\n","    l2_lambda: float = 1e-4,\n","    dropout_rate: float = 0.2,\n","    hidden_units: Tuple[int, int] = (64, 32),\n","    lr: float = 1e-3,\n","    optimizer_name: str = \"adam\"\n",") -> Model:\n","    \"\"\"MLP con 2 capas ocultas mínimo, L2 + Dropout.\"\"\"\n","    reg = regularizers.l2(l2_lambda) if l2_lambda and l2_lambda > 0 else None\n","\n","    inputs = layers.Input(shape=(n_features,))\n","    x = layers.Dense(hidden_units[0], activation=activation, kernel_regularizer=reg)(inputs)\n","    x = layers.Dropout(dropout_rate)(x)\n","    x = layers.Dense(hidden_units[1], activation=activation, kernel_regularizer=reg)(x)\n","    x = layers.Dropout(dropout_rate)(x)\n","    outputs = layers.Dense(1, activation=\"linear\")(x)\n","\n","    model = Model(inputs, outputs, name=\"NatalidadRegressor\")\n","\n","    if optimizer_name.lower() == \"adam\":\n","        opt = optimizers.Adam(learning_rate=lr)\n","    elif optimizer_name.lower() == \"sgd\":\n","        opt = optimizers.SGD(learning_rate=lr, momentum=0.9)\n","    elif optimizer_name.lower() == \"rmsprop\":\n","        opt = optimizers.RMSprop(learning_rate=lr)\n","    else:\n","        opt = optimizers.Adam(learning_rate=lr)\n","\n","    model.compile(optimizer=opt, loss=\"mse\", metrics=[\"mae\"])\n","    return model\n","\n","\n","def train_and_evaluate(\n","    X_train: np.ndarray, y_train: np.ndarray,\n","    X_val: np.ndarray, y_val: np.ndarray,\n","    X_test: np.ndarray, y_test: np.ndarray,\n","    config: Dict[str, Any],\n","    run_name: str\n",") -> Dict[str, Any]:\n","    \"\"\"Entrena un modelo con EarlyStopping/ReduceLROnPlateau y evalúa en test.\"\"\"\n","    model = build_regression_mlp(\n","        n_features=X_train.shape[1],\n","        activation=config.get(\"activation\", \"relu\"),\n","        l2_lambda=config.get(\"l2_lambda\", 1e-4),\n","        dropout_rate=config.get(\"dropout\", 0.2),\n","        hidden_units=config.get(\"hidden_units\", (64, 32)),\n","        lr=config.get(\"lr\", 1e-3),\n","        optimizer_name=config.get(\"optimizer\", \"adam\"),\n","    )\n","\n","    es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, restore_best_weights=True)\n","    rlrop = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=20, min_lr=1e-6)\n","\n","    hist = model.fit(\n","        X_train, y_train,\n","        validation_data=(X_val, y_val),\n","        epochs=config.get(\"epochs\", 500),\n","        batch_size=config.get(\"batch_size\", 16),\n","        verbose=0,\n","        callbacks=[es, rlrop]\n","    )\n","\n","    preds = model.predict(X_test, verbose=0).reshape(-1)\n","    mae = mean_absolute_error(y_test, preds)\n","    rmse = np.sqrt(np.mean((y_test - preds) ** 2))\n","    r2 = r2_score(y_test, preds)\n","\n","    # Guardar curva de entrenamiento\n","    plt.figure()\n","    plt.plot(hist.history[\"loss\"], label=\"train_loss\")\n","    plt.plot(hist.history[\"val_loss\"], label=\"val_loss\")\n","    plt.title(f\"Pérdida (MSE) - {run_name}\")\n","    plt.xlabel(\"Epoch\"); plt.ylabel(\"MSE\")\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(OUT_DIR, f\"loss_curve_{run_name}.png\"), dpi=180)\n","    plt.close()\n","\n","    return {\n","        \"run\": run_name,\n","        \"config\": config,\n","        \"mae\": float(mae),\n","        \"rmse\": float(rmse),\n","        \"r2\": float(r2),\n","        \"model\": model\n","    }"],"metadata":{"id":"gcBUFi38-Enq","executionInfo":{"status":"ok","timestamp":1757631019983,"user_tz":180,"elapsed":169,"user":{"displayName":"Ignacio Verdugo Montecinos","userId":"08266715628392896686"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# ====================================\n","# 3) IMPORTANCIA (PERMUTATION IMPORT.)\n","# ====================================\n","\n","def permutation_importance_mae(\n","    model: Model,\n","    X_test: np.ndarray,\n","    y_test: np.ndarray,\n","    feature_names: List[str],\n","    n_repeats: int = 8,\n","    random_state: int = RNG\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Importancia por permutación medida como incremento de MAE al permutar cada feature.\n","    \"\"\"\n","    rng = np.random.default_rng(random_state)\n","    base_pred = model.predict(X_test, verbose=0).reshape(-1)\n","    base_mae = mean_absolute_error(y_test, base_pred)\n","\n","    results = []\n","    X_work = X_test.copy()\n","    for j, name in enumerate(feature_names):\n","        incs = []\n","        for _ in range(n_repeats):\n","            X_perm = X_work.copy()\n","            rng.shuffle(X_perm[:, j])  # permuta columna j\n","            pred_perm = model.predict(X_perm, verbose=0).reshape(-1)\n","            mae_perm = mean_absolute_error(y_test, pred_perm)\n","            incs.append(mae_perm - base_mae)\n","        results.append({\"feature\": name, \"mae_increase_mean\": float(np.mean(incs)),\n","                        \"mae_increase_std\": float(np.std(incs))})\n","    imp_df = pd.DataFrame(results).sort_values(\"mae_increase_mean\", ascending=False).reset_index(drop=True)\n","    return imp_df"],"metadata":{"id":"JdVtFD4H-LRG","executionInfo":{"status":"ok","timestamp":1757631019986,"user_tz":180,"elapsed":12,"user":{"displayName":"Ignacio Verdugo Montecinos","userId":"08266715628392896686"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# ======================\n","# 4) PROGRAMA PRINCIPAL\n","# ======================\n","\n","def main():\n","    # Carga\n","    df, target = load_dataset(CSV_PATH)\n","    basic_eda(df, target)\n","\n","    # Features y objetivo\n","    feature_cols = [c for c in df.columns if c != target]\n","    num_cols = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n","    X_all = df[num_cols].values.astype(np.float32)\n","    y_all = df[target].values.astype(np.float32).reshape(-1)\n","\n","    # Split train/val/test: 70/15/15 (estricto)\n","    X_temp, X_test, y_temp, y_test = train_test_split(X_all, y_all, test_size=0.15, random_state=RNG)\n","    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1765, random_state=RNG)  # 0.1765*0.85≈0.15\n","\n","    # Preprocesador\n","    pre = make_preprocessor(num_cols)\n","    X_train_p = pre.fit_transform(X_train)\n","    X_val_p = pre.transform(X_val)\n","    X_test_p = pre.transform(X_test)\n","\n","    # Configuraciones a comparar (activaciones y learning rates; regularización fija de base)\n","    configs = []\n","    for act in [\"relu\", \"tanh\", \"elu\"]:\n","        for lr in [1e-3, 5e-4]:\n","            configs.append({\n","                \"activation\": act,\n","                \"lr\": lr,\n","                \"optimizer\": \"adam\",\n","                \"l2_lambda\": 1e-4,\n","                \"dropout\": 0.2,\n","                \"hidden_units\": (64, 32),\n","                \"epochs\": 500,\n","                \"batch_size\": 16\n","            })\n","\n","    results = []\n","    best = None\n","    for i, cfg in enumerate(configs, start=1):\n","        run_name = f\"run{i}_{cfg['activation']}_lr{cfg['lr']}\"\n","        res = train_and_evaluate(X_train_p, y_train, X_val_p, y_val, X_test_p, y_test, cfg, run_name)\n","        results.append(res)\n","        if (best is None) or (res[\"rmse\"] < best[\"rmse\"]):\n","            best = res\n","\n","    # Guardar resultados agregados (sin el objeto modelo)\n","    results_to_save = [\n","        {k: v for k, v in r.items() if k not in (\"model\",)}\n","        for r in results\n","    ]\n","    with open(os.path.join(OUT_DIR, \"resultados_experimentos.json\"), \"w\", encoding=\"utf-8\") as f:\n","        json.dump(results_to_save, f, ensure_ascii=False, indent=2)\n","\n","    # ==========\n","    # 5) ANÁLISIS\n","    # ==========\n","\n","    best_model = best[\"model\"]\n","    # Importancia por permutación (en espacio preprocesado, nombres = num_cols)\n","    imp_df = permutation_importance_mae(best_model, X_test_p, y_test, feature_names=num_cols, n_repeats=16)\n","    imp_df.to_csv(os.path.join(OUT_DIR, \"importancia_variables_perm.csv\"), index=False, encoding=\"utf-8\")\n","\n","    # Predicciones vs reales (test)\n","    y_pred = best_model.predict(X_test_p, verbose=0).reshape(-1)\n","    # Scatter y = ŷ\n","    plt.figure()\n","    plt.scatter(y_test, y_pred)\n","    lims = [min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())]\n","    plt.plot(lims, lims)\n","    plt.title(f\"Predicción vs Real (Mejor modelo) — R²={best['r2']:.3f}\")\n","    plt.xlabel(\"Real (tasa de natalidad)\")\n","    plt.ylabel(\"Predicción (tasa de natalidad)\")\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(OUT_DIR, \"scatter_pred_vs_real.png\"), dpi=180)\n","    plt.close()\n","\n","    # Residuales\n","    residuals = y_test - y_pred\n","    plt.figure()\n","    plt.scatter(y_pred, residuals)\n","    plt.axhline(0)\n","    plt.title(\"Residuales vs Predicción (Mejor modelo)\")\n","    plt.xlabel(\"Predicción\"); plt.ylabel(\"Residuo\")\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(OUT_DIR, \"residuales_vs_prediccion.png\"), dpi=180)\n","    plt.close()\n","\n","    # =============================\n","    # 6) REPORTE Y COMPRESIÓN (ZIP)\n","    # =============================\n","\n","    # Reporte con reflexiones (se basa en métricas y top variables)\n","    top_vars_txt = \", \".join(imp_df.head(5)[\"feature\"].tolist())\n","    report_lines = []\n","    report_lines.append(\"EVALUACIÓN FINAL M8 — RED NEURONAL PARA PREDICCIÓN DE NATALIDAD\\n\")\n","    report_lines.append(\"1) Diseño y entrenamiento\\n\")\n","    report_lines.append(f\"- Mejor configuración: activación={best['config']['activation']}, lr={best['config']['lr']}, \"\n","                        f\"L2={best['config']['l2_lambda']}, dropout={best['config']['dropout']}, capas={best['config']['hidden_units']}\\n\")\n","    report_lines.append(f\"- Métricas (test): MAE={best['mae']:.3f}, RMSE={best['rmse']:.3f}, R²={best['r2']:.3f}\\n\")\n","\n","    report_lines.append(\"2) Evaluación/Comparación\\n\")\n","    report_lines.append(\"- Se compararon activaciones (ReLU, Tanh, ELU) y tasas de aprendizaje (1e-3, 5e-4) con Adam. \"\n","                        \"Se emplearon EarlyStopping y ReduceLROnPlateau para estabilidad y evitar sobreajuste.\\n\")\n","\n","    report_lines.append(\"3) Importancia de variables (permuta, ΔMAE)\\n\")\n","    report_lines.append(f\"- Más influyentes (top-5): {top_vars_txt}\\n\")\n","\n","    report_lines.append(\"4) Reflexión\\n\")\n","    report_lines.append(\"- Las variables con mayor ΔMAE influyen más en la predicción: se relacionan con determinantes \"\n","                        \"socioeconómicos/estructurales (p. ej., ingreso, educación, salud, urbanización, edad media de maternidad). \"\n","                        \"La red captura efectos no lineales y posibles interacciones.\\n\")\n","    report_lines.append(\"- Mejoras futuras: búsqueda de hiperparámetros más amplia (capas/unidades), \"\n","                        \"regularización elástica, k-fold CV, y análisis SHAP para interpretabilidad más fina.\\n\")\n","    report_text = \"\\n\".join(report_lines)\n","\n","    with open(os.path.join(OUT_DIR, \"reporte_final_m8.txt\"), \"w\", encoding=\"utf-8\") as f:\n","        f.write(report_text)\n","\n","    # Crear ZIP con archivos clave\n","    with zipfile.ZipFile(ZIP_PATH, \"w\", zipfile.ZIP_DEFLATED) as z:\n","        # EDA\n","        z.write(os.path.join(OUT_DIR, \"eda_summary.json\"), arcname=\"eda_summary.json\")\n","        if os.path.exists(os.path.join(OUT_DIR, \"correlaciones.png\")):\n","            z.write(os.path.join(OUT_DIR, \"correlaciones.png\"), arcname=\"correlaciones.png\")\n","        # Curvas de pérdida\n","        for file in os.listdir(OUT_DIR):\n","            if file.startswith(\"loss_curve_\") and file.endswith(\".png\"):\n","                z.write(os.path.join(OUT_DIR, file), arcname=file)\n","        # Figuras de evaluación\n","        for fn in [\"scatter_pred_vs_real.png\", \"residuales_vs_prediccion.png\"]:\n","            fp = os.path.join(OUT_DIR, fn)\n","            if os.path.exists(fp):\n","                z.write(fp, arcname=fn)\n","        # Importancia + resultados\n","        for fn in [\"importancia_variables_perm.csv\", \"resultados_experimentos.json\", \"reporte_final_m8.txt\"]:\n","            fp = os.path.join(OUT_DIR, fn)\n","            if os.path.exists(fp):\n","                z.write(fp, arcname=fn)\n","\n","    # Consola\n","    print(\"=== LISTO ===\")\n","    print(f\"Mejor modelo -> MAE={best['mae']:.3f} | RMSE={best['rmse']:.3f} | R²={best['r2']:.3f}\")\n","    print(f\"Top variables (permuta): {', '.join(imp_df.head(5)['feature'].tolist())}\")\n","    print(f\"Carpeta de salida: {os.path.abspath(OUT_DIR)}\")\n","    print(f\"ZIP de entrega:    {os.path.abspath(ZIP_PATH)}\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0lQuS3uG-IRo","executionInfo":{"status":"ok","timestamp":1757631265974,"user_tz":180,"elapsed":245987,"user":{"displayName":"Ignacio Verdugo Montecinos","userId":"08266715628392896686"}},"outputId":"ad9432ed-804a-4555-e502-aa0cc6831b3b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7ac0365a9ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7ac036b6b600> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["=== LISTO ===\n","Mejor modelo -> MAE=1.736 | RMSE=1.854 | R²=0.955\n","Top variables (permuta): pib_per_capita, urbanizacion, edad_maternidad, tasa_empleo_femenino, nivel_educativo\n","Carpeta de salida: /content/m8_output\n","ZIP de entrega:    /content/entrega_final_m8.zip\n"]}]}]}