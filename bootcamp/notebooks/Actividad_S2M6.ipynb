{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM58RKyo1eCM8iWEnou+STe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cd766433"},"source":["## Niveles de ajuste de un modelo\n","\n","### Sobreajuste (Overfitting)\n","\n","El sobreajuste ocurre cuando un modelo aprende los datos de entrenamiento de manera demasiado específica, capturando no solo los patrones importantes sino también el ruido y las peculiaridades de esos datos. Como resultado, el modelo se desempeña muy bien en los datos de entrenamiento, pero falla al generalizar a datos nuevos e invisibles. Es como si el modelo memorizara las respuestas en lugar de entender el concepto.\n","\n","**Ejemplo práctico:** Imagina que estás entrenando un modelo para reconocer si una imagen contiene un gato o un perro. Si el modelo está sobreajustado, podría aprender a identificar un gato basándose en detalles muy específicos de las imágenes de entrenamiento, como la ubicación exacta de una mancha o la forma particular de un bigote en una imagen específica. Cuando se le presenta una nueva imagen de un gato con ligeras variaciones, el modelo podría no reconocerlo correctamente porque esos detalles específicos no están presentes.\n","\n","### Subajuste (Underfitting)\n","\n","El subajuste ocurre cuando un modelo es demasiado simple para capturar los patrones subyacentes en los datos de entrenamiento. El modelo no logra aprender las relaciones importantes entre las características y la variable objetivo, resultando en un rendimiento pobre tanto en los datos de entrenamiento como en los datos nuevos. Es como si el modelo no tuviera suficiente capacidad para entender la complejidad del problema.\n","\n","**Ejemplo práctico:** Volviendo al ejemplo de reconocimiento de imágenes de gatos y perros, un modelo subajustado podría ser un modelo muy básico que solo considera el color promedio de la imagen. Si el color promedio de los gatos de entrenamiento es similar al de los perros de entrenamiento, el modelo no podrá distinguir entre ellos y tendrá un rendimiento deficiente en ambos conjuntos de datos.\n","\n","### Ajuste apropiado (Good Fit)\n","\n","El ajuste apropiado se logra cuando un modelo es capaz de capturar los patrones importantes en los datos de entrenamiento sin memorizar el ruido. El modelo generaliza bien a datos nuevos e invisibles, logrando un buen equilibrio entre el rendimiento en los datos de entrenamiento y en los datos de prueba.\n","\n","**Ejemplo práctico:** Un modelo con ajuste apropiado para el reconocimiento de imágenes de gatos y perros aprendería las características generales que distinguen a los gatos de los perros, como la forma de las orejas, la estructura facial o el cuerpo. Este modelo sería capaz de identificar correctamente gatos y perros en nuevas imágenes, incluso si presentan variaciones respecto a las imágenes de entrenamiento, porque ha aprendido los patrones relevantes en lugar de detalles específicos."]},{"cell_type":"markdown","metadata":{"id":"61e7f94a"},"source":["## Trade-off entre sesgo y varianza\n","\n","En el contexto del aprendizaje automático, el **sesgo** y la **varianza** son dos fuentes de error que impiden que los algoritmos de aprendizaje generalicen más allá de sus datos de entrenamiento. Existe un **trade-off** (intercambio) inherente entre estos dos errores en la construcción de un modelo predictivo.\n","\n","*   **Sesgo (Bias):** Es el error introducido por aproximar un problema de la vida real, que puede ser complicado, por un modelo mucho más simple. Un alto sesgo significa que el modelo hace suposiciones incorrectas sobre la relación entre las características y la variable objetivo. Un modelo con alto sesgo tiende a ser demasiado simple y a **subajustar** (underfitting) los datos. No captura los patrones importantes.\n","\n","*   **Varianza (Variance):** Es la sensibilidad del modelo a pequeñas fluctuaciones en el conjunto de entrenamiento. Un alto varianza significa que el modelo aprende demasiado del ruido en los datos de entrenamiento y no generaliza bien a datos nuevos e invisibles. Un modelo con alta varianza tiende a ser demasiado complejo y a **sobreajustar** (overfitting) los datos. Captura el ruido además de los patrones.\n","\n","**Relación con el sobreajuste y subajuste:**\n","\n","*   **Subajuste (Alto Sesgo, Baja Varianza):** Un modelo subajustado es demasiado simple. Tiene un alto sesgo porque no puede capturar la complejidad de los datos. Su varianza es baja porque no es sensible a las pequeñas variaciones en los datos de entrenamiento; simplemente no aprende lo suficiente.\n","\n","*   **Sobreajuste (Bajo Sesgo, Alta Varianza):** Un modelo sobreajustado es demasiado complejo. Tiene un bajo sesgo porque puede modelar la relación entre las características y la variable objetivo en los datos de entrenamiento de manera muy precisa. Sin embargo, su varianza es alta porque es demasiado sensible al ruido en los datos de entrenamiento y no generaliza bien a datos nuevos.\n","\n","*   **Ajuste Apropiado (Equilibrio entre Sesgo y Varianza):** El objetivo es encontrar un modelo que tenga un equilibrio entre sesgo y varianza. Este modelo será lo suficientemente complejo para capturar los patrones importantes en los datos, pero no tan complejo como para memorizar el ruido.\n","\n","**Ejemplo de cómo equilibrar el sesgo y la varianza:**\n","\n","Imagina que estás construyendo un modelo para predecir el precio de una casa basado en su tamaño.\n","\n","*   **Modelo con alto sesgo (subajuste):** Un modelo lineal simple (una línea recta) podría tener un alto sesgo si la relación entre el tamaño y el precio no es estrictamente lineal. Ignoraría las variaciones en los precios debido a otros factores como la ubicación o el número de habitaciones. Este modelo subajustaría los datos.\n","\n","*   **Modelo con alta varianza (sobreajuste):** Un modelo muy complejo que intenta ajustar cada punto de datos de entrenamiento perfectamente (por ejemplo, un polinomio de alto grado) tendría una alta varianza. Sería muy sensible a cada pequeña variación en los precios de las casas de entrenamiento, incluyendo el ruido. Este modelo sobreajustaría los datos y no predeciría bien los precios de las casas nuevas.\n","\n","*   **Equilibrando sesgo y varianza:** Podrías usar un modelo más flexible que una línea recta pero menos complejo que un polinomio de alto grado, como un polinomio de grado 2 o 3, o considerar la inclusión de otras características (ubicación, número de habitaciones) en un modelo lineal. Técnicas como la **validación cruzada** te ayudarían a evaluar el rendimiento del modelo en datos no vistos y ajustar la complejidad del modelo para encontrar el mejor equilibrio entre sesgo y varianza. Si el modelo tiene un alto rendimiento en los datos de entrenamiento pero bajo en los datos de validación, es probable que esté sobreajustando y necesite simplificarse (aumentar sesgo, disminuir varianza). Si el rendimiento es bajo en ambos conjuntos, es probable que esté subajustando y necesite ser más complejo (disminuir sesgo, aumentar varianza)."]},{"cell_type":"markdown","metadata":{"id":"c987c3dd"},"source":["## Validación cruzada\n","\n","La **validación cruzada** es una técnica fundamental en el aprendizaje automático para evaluar el rendimiento de un modelo predictivo y determinar qué tan bien generalizará a datos nuevos e invisibles. Su importancia radica en que ayuda a mitigar los problemas de sobreajuste y subajuste al proporcionar una estimación más robusta del rendimiento del modelo que simplemente dividir los datos una vez.\n","\n","**¿Por qué es importante?**\n","\n","*   **Estimación más fiable del rendimiento:** En lugar de depender de una única división de datos (entrenamiento/prueba), la validación cruzada utiliza múltiples divisiones, lo que reduce la varianza de la estimación del error del modelo.\n","*   **Detección de sobreajuste:** Al evaluar el modelo en diferentes subconjuntos de datos, la validación cruzada ayuda a identificar si el modelo se está ajustando demasiado a los datos de entrenamiento específicos.\n","*   **Selección de modelos y ajuste de hiperparámetros:** Permite comparar diferentes modelos o diferentes conjuntos de hiperparámetros para un mismo modelo basándose en su rendimiento promedio en las diferentes iteraciones de validación cruzada.\n","*   **Uso eficiente de los datos:** Especialmente útil con conjuntos de datos pequeños, ya que permite que todos los puntos de datos se utilicen tanto para entrenamiento como para validación en algún momento.\n","\n","**Técnicas de validación cruzada:**\n","\n","*   **Método de retención (Hold-Out):** Es la forma más simple. El conjunto de datos se divide en dos subconjuntos: un conjunto de entrenamiento para entrenar el modelo y un conjunto de prueba para evaluarlo. La división suele ser un porcentaje fijo, por ejemplo, 80% para entrenamiento y 20% para prueba. Es rápido y sencillo, pero la estimación del rendimiento puede ser muy sensible a la forma en que se realiza la división.\n","\n","*   **Validación cruzada de k-iteraciones (k-Fold):** El conjunto de datos se divide en $k$ subconjuntos (folds) de tamaño aproximadamente igual. En cada una de las $k$ iteraciones, uno de los folds se utiliza como conjunto de prueba y los $k-1$ folds restantes se utilizan como conjunto de entrenamiento. El proceso se repite $k$ veces, asegurando que cada fold se utilice como conjunto de prueba exactamente una vez. El rendimiento del modelo se promedia sobre las $k$ iteraciones. Es una técnica muy común y proporciona una estimación más estable del rendimiento que el Hold-Out.\n","\n","*   **Validación cruzada aleatoria (Random Subsampling):** Similar al Hold-Out, pero se repite varias veces con diferentes divisiones aleatorias del conjunto de datos en entrenamiento y prueba. El rendimiento del modelo se promedia sobre todas las repeticiones. Permite controlar el tamaño de los conjuntos de entrenamiento y prueba de forma independiente del número de repeticiones, pero algunos puntos de datos pueden no ser seleccionados para el conjunto de prueba en ninguna iteración, y otros pueden ser seleccionados varias veces.\n","\n","*   **Validación cruzada dejando uno afuera (Leave-One-Out):** Es un caso especial de k-Fold donde $k$ es igual al número de instancias en el conjunto de datos. En cada iteración, se utiliza una única instancia como conjunto de prueba y todas las demás instancias se utilizan como conjunto de entrenamiento. El proceso se repite para cada instancia. Es computacionalmente costoso para conjuntos de datos grandes, pero proporciona una estimación casi insesgada del rendimiento del modelo. Sin embargo, puede tener una alta varianza porque las iteraciones de entrenamiento son muy similares entre sí."]},{"cell_type":"markdown","metadata":{"id":"da329abd"},"source":["## Implementación con Scikit-Learn\n","\n","La validación cruzada se implementa de manera muy sencilla utilizando la librería Scikit-Learn en Python. Scikit-Learn proporciona varias herramientas para realizar diferentes tipos de validación cruzada.\n","\n","Para implementar la validación cruzada con Scikit-Learn, generalmente se siguen estos pasos:\n","\n","1.  **Importar las funciones o clases necesarias:** Dependiendo del tipo de validación cruzada que quieras realizar y de cómo quieras evaluar el modelo, importarás diferentes módulos de `sklearn.model_selection`.\n","2.  **Cargar o preparar tus datos:** Asegúrate de tener tus datos de características (X) y la variable objetivo (y) listos.\n","3.  **Instanciar el modelo:** Crea una instancia del modelo de aprendizaje automático que deseas evaluar (por ejemplo, `LogisticRegression`, `SVC`, `RandomForestClassifier`, etc.).\n","4.  **Aplicar la función de validación cruzada:** Utiliza una de las funciones de validación cruzada de Scikit-Learn, pasando el modelo, los datos y el número de folds (para k-Fold) o el iterador de validación cruzada deseado.\n","\n","### Funciones o clases de Scikit-Learn para validación cruzada:\n","\n","Aquí se mencionan al menos dos funciones o clases comunes utilizadas para la validación cruzada en Scikit-Learn:\n","\n","*   **`cross_val_score`:** Esta es una de las funciones más utilizadas para realizar validación cruzada de k-iteraciones. Evalúa un estimador utilizando validación cruzada y devuelve una puntuación para cada fold. Es útil para una evaluación rápida del rendimiento del modelo."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e6bef0c4","executionInfo":{"status":"ok","timestamp":1757090065538,"user_tz":240,"elapsed":3686,"user":{"displayName":"Ignacio Verdugo Montecinos","userId":"08266715628392896686"}},"outputId":"f4e836b7-cb2a-4109-cebf-7eb000543885"},"source":["    from sklearn.model_selection import KFold\n","    from sklearn.linear_model import LogisticRegression\n","    from sklearn.datasets import load_iris\n","    from sklearn.metrics import accuracy_score\n","\n","    # Cargar datos de ejemplo\n","    X, y = load_iris(return_X_y=True)\n","\n","    # Instanciar el modelo\n","    model = LogisticRegression(max_iter=200)\n","\n","    # Configurar validación cruzada de 5 folds\n","    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","    # Iterar sobre los folds\n","    fold_accuracies = []\n","    for train_index, test_index in kf.split(X):\n","        X_train, X_test = X[train_index], X[test_index]\n","        y_train, y_test = y[train_index], y[test_index]\n","\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_test)\n","        accuracy = accuracy_score(y_test, y_pred)\n","        fold_accuracies.append(accuracy)\n","\n","    print(\"Precisión por fold:\", fold_accuracies)\n","    print(\"Precisión media:\", sum(fold_accuracies) / len(fold_accuracies))"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Precisión por fold: [1.0, 1.0, 0.9333333333333333, 0.9666666666666667, 0.9666666666666667]\n","Precisión media: 0.9733333333333334\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"xB9am8Qcxrwb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"009b3e95"},"source":["## Análisis del Caso de Estudio: Predicción de Precios de Viviendas\n","\n","En este caso, el equipo de científicos de datos enfrenta un modelo de predicción de precios de viviendas con rendimiento inconsistente, lo que sugiere un posible problema de sobreajuste o subajuste.\n","\n","**1. ¿Cómo podrían diagnosticar si el modelo está sobreajustado o subajustado? Describe los pasos que seguirías para identificar el problema.**\n","\n","Para diagnosticar si el modelo está sobreajustado o subajustado, el equipo debería evaluar el rendimiento del modelo en dos conjuntos de datos distintos: el conjunto de entrenamiento y un conjunto de validación (o prueba). Los pasos serían los siguientes:\n","\n","*   **Dividir los datos:** Primero, dividirían el conjunto de datos disponible en al menos dos partes: un conjunto de entrenamiento (usado para entrenar el modelo) y un conjunto de validación o prueba (usado para evaluar el modelo en datos no vistos durante el entrenamiento). Una división común es 80% para entrenamiento y 20% para validación/prueba.\n","*   **Entrenar el modelo:** Entrenarían el modelo de predicción de precios de viviendas utilizando únicamente el conjunto de entrenamiento.\n","*   **Evaluar el rendimiento:** Calcularían una métrica de rendimiento adecuada para la predicción de precios (un problema de regresión), como el Error Cuadrático Medio (MSE), el Error Absoluto Medio (MAE) o el Coeficiente de Determinación (R²). Evaluarían el modelo tanto en el conjunto de entrenamiento como en el conjunto de validación/prueba.\n","*   **Comparar el rendimiento:** Analizarían los resultados de rendimiento en ambos conjuntos:\n","    *   **Sobreajuste:** Si el modelo tiene un rendimiento significativamente mejor en el conjunto de entrenamiento que en el conjunto de validación/prueba, indica sobreajuste. El modelo ha memorizado los datos de entrenamiento y no generaliza bien a datos nuevos.\n","    *   **Subajuste:** Si el modelo tiene un rendimiento pobre tanto en el conjunto de entrenamiento como en el conjunto de validación/prueba, indica subajuste. El modelo es demasiado simple y no ha logrado capturar los patrones relevantes en los datos.\n","    *   **Ajuste Apropiado:** Si el rendimiento es similar y bueno en ambos conjuntos, el modelo probablemente tiene un ajuste apropiado.\n","\n","**2. ¿Qué técnica de validación cruzada recomendarías para evaluar el rendimiento del modelo y por qué?**\n","\n","Recomendaría la **validación cruzada de k-iteraciones (k-Fold)** para evaluar el rendimiento del modelo.\n","\n","*   **Por qué:**\n","    *   Proporciona una estimación más robusta y fiable del rendimiento del modelo que una simple división Hold-Out, ya que el modelo se entrena y evalúa en múltiples subconjuntos de datos.\n","    *   Reduce la varianza de la estimación del error, lo que significa que la estimación del rendimiento es menos sensible a la forma particular en que se dividen los datos.\n","    *   Utiliza todos los datos disponibles tanto para entrenamiento como para validación en algún momento, lo cual es eficiente, especialmente si el conjunto de datos no es extremadamente grande.\n","    *   Ayuda a detectar el sobreajuste al mostrar cómo el modelo se comporta en diferentes particiones de los datos.\n","\n","Un valor común para $k$ es 5 o 10, pero puede ajustarse dependiendo del tamaño del conjunto de datos.\n","\n","**3. ¿Cómo podrían utilizar la validación cruzada para ajustar los hiperparámetros del modelo y mejorar su rendimiento?**\n","\n","La validación cruzada es fundamental para el ajuste de hiperparámetros. El equipo podría utilizarla de la siguiente manera:\n","\n","*   **Definir una cuadrícula de hiperparámetros:** Identificar los hiperparámetros clave del modelo que desean ajustar (por ejemplo, la complejidad del modelo, la tasa de aprendizaje, el número de estimadores en un modelo de ensamble, etc.) y definir un rango o una lista de valores posibles para cada hiperparámetro.\n","*   **Utilizar Grid Search o Random Search con validación cruzada:** Scikit-Learn ofrece herramientas como `GridSearchCV` o `RandomizedSearchCV` que automatizan este proceso. Estas herramientas funcionan de la siguiente manera:\n","    *   Para cada combinación de hiperparámetros (en Grid Search) o para un número determinado de combinaciones aleatorias (en Random Search), se entrena y evalúa el modelo utilizando validación cruzada (por ejemplo, k-Fold) en el conjunto de entrenamiento.\n","    *   Se calcula la puntuación media de rendimiento (utilizando una métrica apropiada como MSE o R²) a través de los folds para cada combinación de hiperparámetros.\n","    *   La herramienta selecciona la combinación de hiperparámetros que produjo la mejor puntuación media de validación cruzada.\n","*   **Entrenar el modelo final:** Una vez que se ha encontrado la mejor combinación de hiperparámetros utilizando la validación cruzada en el conjunto de entrenamiento, se entrena el modelo final en *todo* el conjunto de entrenamiento (con los hiperparámetros óptimos encontrados).\n","*   **Evaluación final:** Finalmente, se evalúa el rendimiento del modelo final (entrenado en todo el conjunto de entrenamiento con los hiperparámetros óptimos) en el conjunto de *prueba* (el que se reservó al principio y que el modelo nunca ha visto). Esta evaluación final proporciona una estimación imparcial del rendimiento del modelo en datos completamente nuevos.\n","\n","**4. ¿Qué beneficios y desafíos podrían enfrentar al implementar la validación cruzada en este caso?**\n","\n","**Beneficios:**\n","\n","*   **Estimación de rendimiento más fiable:** Como se mencionó, proporciona una medida más precisa de qué tan bien generalizará el modelo a datos no vistos, reduciendo la probabilidad de sorpresas en producción.\n","*   **Mejor detección de sobreajuste:** Permite identificar si el modelo se está ajustando demasiado a subconjuntos específicos de datos.\n","*   **Optimización de hiperparámetros más efectiva:** Ayuda a seleccionar los hiperparámetros que mejor generalizan, en lugar de aquellos que solo funcionan bien en una única división de entrenamiento/validación.\n","*   **Uso eficiente de datos:** Maximiza el uso de los datos disponibles para entrenamiento y validación.\n","\n","**Desafíos:**\n","\n","*   **Costo computacional:** La validación cruzada requiere entrenar y evaluar el modelo múltiples veces ($k$ veces para k-Fold). Para modelos complejos o conjuntos de datos muy grandes, esto puede ser computacionalmente costoso y llevar mucho tiempo.\n","*   **Interpretación de resultados:** Aunque la puntuación media es un buen indicador, es importante también observar la variabilidad de las puntuaciones entre los folds para entender la estabilidad del modelo.\n","*   **Selección de k:** Elegir el número adecuado de folds ($k$) puede requerir cierta experimentación. Un $k$ muy pequeño podría tener un sesgo alto, mientras que un $k$ muy grande (como Leave-One-Out) puede tener una varianza alta y ser muy costoso computacionalmente.\n","*   **Manejo de datos dependientes del tiempo o con estructura:** Si los datos de precios de viviendas tienen una dependencia temporal o alguna otra estructura particular (por ejemplo, datos geoespaciales agrupados), se necesitarían estrategias de validación cruzada más avanzadas (como Time Series Cross-Validation o GroupKFold) para evitar la fuga de información y obtener una estimación de rendimiento realista."]},{"cell_type":"code","source":[],"metadata":{"id":"6UPIgnKdyq_0"},"execution_count":null,"outputs":[]}]}